{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASL Sign Language Recognition - FRESH START\n",
    "### Real-time Hand Detection with MediaPipe + CNN\n",
    "\n",
    "**IMPORTANT: Run cells IN ORDER. Do NOT skip cells!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîß STEP 1: Clean Installation\n",
    "**Run this cell, then RESTART RUNTIME when it finishes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete fresh installation\n",
    "!pip install mediapipe opencv-python kaggle --quiet\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"‚úÖ INSTALLATION COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nüî¥ CRITICAL: You MUST restart runtime now!\")\n",
    "print(\"\\nüìç Click: Runtime ‚Üí Restart runtime\")\n",
    "print(\"üìç Or press: Ctrl+M then press .\")\n",
    "print(\"\\n‚ö†Ô∏è  After restart, run STEP 2 below\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìö STEP 2: Import All Libraries\n",
    "**Run this AFTER restarting runtime**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from google.colab.patches import cv2_imshow\n",
    "from google.colab import files\n",
    "import pickle\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"‚úÖ ALL LIBRARIES IMPORTED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüì¶ TensorFlow: {tf.__version__}\")\n",
    "print(f\"üì¶ MediaPipe: {mp.__version__}\")\n",
    "print(f\"üì¶ NumPy: {np.__version__}\")\n",
    "print(\"\\n‚úÖ Ready to continue! Run the next cells in order.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìÇ STEP 3: Upload Kaggle API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìÅ Upload your kaggle.json file\")\n",
    "print(\"\\nüîó Get it from: https://www.kaggle.com/settings\")\n",
    "print(\"   ‚Üí Click 'Create New Token' under API section\\n\")\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Setup Kaggle\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "print(\"\\n‚úÖ Kaggle API configured successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üì• STEP 4: Download ASL Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üì• Downloading ASL Alphabet Dataset...\\n\")\n",
    "print(\"‚è≥ This will take 2-3 minutes...\\n\")\n",
    "\n",
    "!kaggle datasets download -d grassknoted/asl-alphabet --quiet\n",
    "\n",
    "print(\"\\nüì¶ Extracting dataset...\")\n",
    "!unzip -q asl-alphabet.zip -d asl_data\n",
    "\n",
    "print(\"\\n‚úÖ Dataset ready!\")\n",
    "print(\"\\nüìä Available classes:\")\n",
    "!ls asl_data/asl_alphabet_train/asl_alphabet_train/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ü§ñ STEP 5: Initialize MediaPipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "print(\"‚úÖ MediaPipe Hands initialized!\")\n",
    "print(\"\\n‚öôÔ∏è  Configuration:\")\n",
    "print(\"   - Max hands: 1\")\n",
    "print(\"   - Detection confidence: 50%\")\n",
    "print(\"   - Tracking confidence: 50%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîç STEP 6: Define Hand Landmark Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_landmarks(image_path):\n",
    "    \"\"\"\n",
    "    Extract 21 hand landmarks (x,y,z) = 63 features total\n",
    "    \"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        return None\n",
    "    \n",
    "    # Convert to RGB\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Process with MediaPipe\n",
    "    results = hands.process(image_rgb)\n",
    "    \n",
    "    if results.multi_hand_landmarks:\n",
    "        # Get first hand\n",
    "        hand = results.multi_hand_landmarks[0]\n",
    "        \n",
    "        # Extract all landmarks\n",
    "        landmarks = []\n",
    "        for lm in hand.landmark:\n",
    "            landmarks.extend([lm.x, lm.y, lm.z])\n",
    "        \n",
    "        return np.array(landmarks)\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(\"‚úÖ Landmark extraction function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä STEP 7: Process Dataset & Extract Features\n",
    "**This will take 5-10 minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'asl_data/asl_alphabet_train/asl_alphabet_train/'\n",
    "classes = sorted(os.listdir(dataset_path))\n",
    "\n",
    "print(f\"üìö Found {len(classes)} classes\\n\")\n",
    "print(f\"Classes: {classes}\\n\")\n",
    "\n",
    "# Storage\n",
    "X = []  # Features\n",
    "y = []  # Labels\n",
    "\n",
    "# Use 300 images per class (increase for better accuracy)\n",
    "IMAGES_PER_CLASS = 300\n",
    "\n",
    "print(f\"‚öôÔ∏è  Processing {IMAGES_PER_CLASS} images per class...\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for class_name in classes:\n",
    "    class_path = os.path.join(dataset_path, class_name)\n",
    "    images = os.listdir(class_path)[:IMAGES_PER_CLASS]\n",
    "    \n",
    "    success_count = 0\n",
    "    \n",
    "    for img_name in images:\n",
    "        img_path = os.path.join(class_path, img_name)\n",
    "        landmarks = extract_landmarks(img_path)\n",
    "        \n",
    "        if landmarks is not None:\n",
    "            X.append(landmarks)\n",
    "            y.append(class_name)\n",
    "            success_count += 1\n",
    "    \n",
    "    print(f\"‚úì '{class_name}': {success_count}/{IMAGES_PER_CLASS} processed\")\n",
    "\n",
    "# Convert to arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n‚úÖ Feature extraction complete!\")\n",
    "print(f\"\\nüìä Total samples: {len(X)}\")\n",
    "print(f\"üìä Feature shape: {X.shape}\")\n",
    "print(f\"üìä Classes: {len(np.unique(y))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ STEP 8: Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "y_categorical = keras.utils.to_categorical(y_encoded)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_categorical, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Data prepared!\\n\")\n",
    "print(f\"üìä Training samples: {len(X_train)}\")\n",
    "print(f\"üìä Testing samples: {len(X_test)}\")\n",
    "print(f\"üìä Number of classes: {len(label_encoder.classes_)}\")\n",
    "\n",
    "# Save label encoder\n",
    "with open('label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "print(\"\\n‚úÖ Label encoder saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üß† STEP 9: Build CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "model = keras.Sequential([\n",
    "    # Input: 63 features (21 landmarks √ó 3 coordinates)\n",
    "    layers.Input(shape=(63,)),\n",
    "    layers.Reshape((21, 3)),  # Reshape to (landmarks, coordinates)\n",
    "    \n",
    "    # Conv1D layers\n",
    "    layers.Conv1D(64, 3, activation='relu', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "    \n",
    "    layers.Conv1D(128, 3, activation='relu', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "    \n",
    "    layers.Conv1D(256, 3, activation='relu', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.GlobalMaxPooling1D(),\n",
    "    \n",
    "    # Dense layers\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    \n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.4),\n",
    "    \n",
    "    # Output layer\n",
    "    layers.Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model built!\\n\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üöÄ STEP 10: Train Model\n",
    "**This will take 5-10 minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=0.00001\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"üöÄ Starting training...\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n=\"*60)\n",
    "print(\"‚úÖ TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìà STEP 11: Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üìä MODEL PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n‚úÖ Test Accuracy: {test_accuracy*100:.2f}%\")\n",
    "print(f\"‚úÖ Test Loss: {test_loss:.4f}\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy\n",
    "ax1.plot(history.history['accuracy'], label='Train', linewidth=2, color='blue')\n",
    "ax1.plot(history.history['val_accuracy'], label='Validation', linewidth=2, color='orange')\n",
    "ax1.set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Accuracy', fontsize=12)\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Loss\n",
    "ax2.plot(history.history['loss'], label='Train', linewidth=2, color='blue')\n",
    "ax2.plot(history.history['val_loss'], label='Validation', linewidth=2, color='orange')\n",
    "ax2.set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Loss', fontsize=12)\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save model\n",
    "model.save('asl_model.h5')\n",
    "print(\"\\n‚úÖ Model saved as 'asl_model.h5'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üì∏ STEP 12: Setup Webcam Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Javascript\n",
    "from google.colab.output import eval_js\n",
    "from base64 import b64decode\n",
    "\n",
    "def take_photo(filename='photo.jpg', quality=0.8):\n",
    "    \"\"\"Capture photo from webcam\"\"\"\n",
    "    js = Javascript('''\n",
    "        async function takePhoto(quality) {\n",
    "            const div = document.createElement('div');\n",
    "            const video = document.createElement('video');\n",
    "            video.style.display = 'block';\n",
    "            const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
    "\n",
    "            document.body.appendChild(div);\n",
    "            div.appendChild(video);\n",
    "            video.srcObject = stream;\n",
    "            await video.play();\n",
    "\n",
    "            google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
    "            await new Promise((resolve) => setTimeout(resolve, 1000));\n",
    "\n",
    "            const canvas = document.createElement('canvas');\n",
    "            canvas.width = video.videoWidth;\n",
    "            canvas.height = video.videoHeight;\n",
    "            canvas.getContext('2d').drawImage(video, 0, 0);\n",
    "            stream.getVideoTracks()[0].stop();\n",
    "            div.remove();\n",
    "            return canvas.toDataURL('image/jpeg', quality);\n",
    "        }\n",
    "    ''')\n",
    "    display(js)\n",
    "    data = eval_js('takePhoto({})'.format(quality))\n",
    "    binary = b64decode(data.split(',')[1])\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(binary)\n",
    "    return filename\n",
    "\n",
    "def predict_sign(image_path):\n",
    "    \"\"\"Predict ASL sign from image\"\"\"\n",
    "    landmarks = extract_landmarks(image_path)\n",
    "    \n",
    "    if landmarks is None:\n",
    "        return None, None, None\n",
    "    \n",
    "    # Predict\n",
    "    landmarks = landmarks.reshape(1, -1)\n",
    "    prediction = model.predict(landmarks, verbose=0)\n",
    "    \n",
    "    # Get results\n",
    "    idx = np.argmax(prediction)\n",
    "    letter = label_encoder.inverse_transform([idx])[0]\n",
    "    confidence = prediction[0][idx]\n",
    "    \n",
    "    return letter, confidence, prediction[0]\n",
    "\n",
    "print(\"‚úÖ Webcam functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ STEP 13: REAL-TIME ASL RECOGNITION!\n",
    "**Run this cell multiple times to test different signs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"üì∏ CAPTURING FROM WEBCAM...\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nüëã Show your ASL sign now!\\n\")\n",
    "\n",
    "# Capture photo\n",
    "photo = take_photo('capture.jpg')\n",
    "\n",
    "# Load and process image\n",
    "image = cv2.imread(photo)\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Draw hand landmarks\n",
    "results = hands.process(image_rgb)\n",
    "if results.multi_hand_landmarks:\n",
    "    for hand_landmarks in results.multi_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image, \n",
    "            hand_landmarks, \n",
    "            mp_hands.HAND_CONNECTIONS,\n",
    "            mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=3),\n",
    "            mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=2)\n",
    "        )\n",
    "\n",
    "# Predict sign\n",
    "letter, confidence, all_predictions = predict_sign(photo)\n",
    "\n",
    "if letter:\n",
    "    # Add text to image\n",
    "    cv2.putText(image, f\"Sign: {letter}\", (10, 50),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 3)\n",
    "    cv2.putText(image, f\"{confidence*100:.1f}%\", (10, 100),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"üéØ PREDICTION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\n‚úÖ Detected Sign: {letter}\")\n",
    "    print(f\"‚úÖ Confidence: {confidence*100:.2f}%\\n\")\n",
    "    \n",
    "    # Top 3 predictions\n",
    "    top3_idx = np.argsort(all_predictions)[-3:][::-1]\n",
    "    print(\"üìä Top 3 Predictions:\")\n",
    "    for i, idx in enumerate(top3_idx, 1):\n",
    "        l = label_encoder.inverse_transform([idx])[0]\n",
    "        c = all_predictions[idx] * 100\n",
    "        print(f\"   {i}. {l}: {c:.2f}%\")\n",
    "    \n",
    "else:\n",
    "    cv2.putText(image, \"NO HAND DETECTED!\", (10, 50),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 3)\n",
    "    print(\"=\"*60)\n",
    "    print(\"‚ùå NO HAND DETECTED\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nüí° Tips:\")\n",
    "    print(\"   - Ensure good lighting\")\n",
    "    print(\"   - Keep hand centered in frame\")\n",
    "    print(\"   - Show your hand clearly\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üì∑ CAPTURED IMAGE:\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "cv2_imshow(image)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üí° Run this cell again to test another sign!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üíæ STEP 14: Download Trained Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üì• Downloading model files...\\n\")\n",
    "\n",
    "files.download('asl_model.h5')\n",
    "files.download('label_encoder.pkl')\n",
    "\n",
    "print(\"\\n‚úÖ Model files downloaded!\")\n",
    "print(\"\\nüì¶ You now have:\")\n",
    "print(\"   - asl_model.h5 (trained model)\")\n",
    "print(\"   - label_encoder.pkl (label mappings)\")\n",
    "print(\"\\nüí° You can use these files later without retraining!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìù Notes\n",
    "\n",
    "### ‚úÖ What This System Does:\n",
    "- Recognizes **static ASL alphabet signs** (A-Z + Space, Delete, Nothing)\n",
    "- Uses **MediaPipe** for hand landmark detection (21 points)\n",
    "- Trains a **CNN** model on hand coordinates\n",
    "- **Real-time prediction** from webcam\n",
    "- Expected accuracy: **90-95%**\n",
    "\n",
    "### üöÄ To Improve Accuracy:\n",
    "1. Increase `IMAGES_PER_CLASS` to 500-1000\n",
    "2. Train for more epochs\n",
    "3. Ensure good lighting during testing\n",
    "4. Keep hand centered and steady\n",
    "\n",
    "### üìπ For Dynamic Signs (Words):\n",
    "Dynamic signs require:\n",
    "- Video dataset with motion sequences\n",
    "- LSTM or GRU architecture\n",
    "- Temporal feature extraction\n",
    "\n",
    "Let me know if you want to extend to dynamic signs!\n",
    "\n",
    "---\n",
    "## üéâ PROJECT COMPLETE!\n",
    "You now have a working ASL recognition system!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

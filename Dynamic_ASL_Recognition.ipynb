{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic ASL Recognition - Words & Phrases\n",
    "### Real-time Recognition of ASL Words with Movement\n",
    "\n",
    "**This extends your static ASL model to recognize dynamic signs (words with movement)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîß STEP 1: Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mediapipe opencv-python kaggle --quiet\n",
    "\n",
    "print(\"‚úÖ Installation complete!\")\n",
    "print(\"\\nüî¥ RESTART RUNTIME: Runtime ‚Üí Restart runtime\")\n",
    "print(\"Then run Step 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìö STEP 2: Import Libraries (After Restart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from google.colab.patches import cv2_imshow\n",
    "from google.colab import files\n",
    "import pickle\n",
    "from IPython.display import display, Javascript, HTML\n",
    "from google.colab.output import eval_js\n",
    "from base64 import b64decode\n",
    "import time\n",
    "\n",
    "print(\"‚úÖ All libraries imported!\")\n",
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "print(f\"MediaPipe: {mp.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìÇ STEP 3: Download Dynamic ASL Dataset\n",
    "\n",
    "We'll use the **WLASL** (Word-Level American Sign Language) dataset or create our own training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìÅ Upload your kaggle.json file\")\n",
    "print(\"Get from: https://www.kaggle.com/settings\\n\")\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "print(\"\\n‚úÖ Kaggle configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this demo, we'll create our own dataset by recording sequences\n",
    "# In production, you'd use a proper video dataset like WLASL\n",
    "\n",
    "print(\"üìä For dynamic signs, we need VIDEO sequences\")\n",
    "print(\"\\nüéØ We'll collect data in real-time for these words:\")\n",
    "print(\"   - HELLO\")\n",
    "print(\"   - THANK YOU\")\n",
    "print(\"   - PLEASE\")\n",
    "print(\"   - YES\")\n",
    "print(\"   - NO\")\n",
    "print(\"\\nüí° Or we can use a pre-recorded dataset...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ü§ñ STEP 4: Initialize MediaPipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=2,  # Support both hands for some signs\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "print(\"‚úÖ MediaPipe initialized for dynamic recognition!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üé¨ STEP 5: Video Capture Function for Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_sequence(num_frames=30, delay_ms=100):\n",
    "    \"\"\"\n",
    "    Capture a sequence of frames (for dynamic signs)\n",
    "    num_frames: Number of frames to capture (default 30 = ~3 seconds at 10 FPS)\n",
    "    delay_ms: Delay between frames in milliseconds\n",
    "    \"\"\"\n",
    "    print(f\"üìπ Capturing {num_frames} frames...\\n\")\n",
    "    \n",
    "    frames = []\n",
    "    \n",
    "    for i in range(num_frames):\n",
    "        # JavaScript to capture frame\n",
    "        js = Javascript('''\n",
    "            async function captureFrame(quality) {\n",
    "                if (!window.stream) {\n",
    "                    const video = document.createElement('video');\n",
    "                    video.style.display = 'block';\n",
    "                    window.stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
    "                    \n",
    "                    const div = document.createElement('div');\n",
    "                    div.id = 'video-container';\n",
    "                    document.body.appendChild(div);\n",
    "                    div.appendChild(video);\n",
    "                    video.srcObject = window.stream;\n",
    "                    await video.play();\n",
    "                    window.video = video;\n",
    "                    \n",
    "                    google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
    "                    await new Promise(resolve => setTimeout(resolve, 1000));\n",
    "                }\n",
    "                \n",
    "                const canvas = document.createElement('canvas');\n",
    "                canvas.width = window.video.videoWidth;\n",
    "                canvas.height = window.video.videoHeight;\n",
    "                canvas.getContext('2d').drawImage(window.video, 0, 0);\n",
    "                return canvas.toDataURL('image/jpeg', quality);\n",
    "            }\n",
    "        ''')\n",
    "        \n",
    "        if i == 0:\n",
    "            display(js)\n",
    "        \n",
    "        data = eval_js('captureFrame(0.8)')\n",
    "        binary = b64decode(data.split(',')[1])\n",
    "        \n",
    "        filename = f'frame_{i}.jpg'\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(binary)\n",
    "        \n",
    "        frames.append(filename)\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"‚úì Captured {i + 1}/{num_frames} frames\")\n",
    "        \n",
    "        time.sleep(delay_ms / 1000)\n",
    "    \n",
    "    # Stop camera\n",
    "    display(Javascript('''\n",
    "        if (window.stream) {\n",
    "            window.stream.getVideoTracks()[0].stop();\n",
    "            document.getElementById('video-container').remove();\n",
    "            window.stream = null;\n",
    "        }\n",
    "    '''))\n",
    "    \n",
    "    print(f\"\\n‚úÖ Captured {num_frames} frames!\")\n",
    "    return frames\n",
    "\n",
    "print(\"‚úÖ Video sequence capture function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîç STEP 6: Extract Landmarks from Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sequence_landmarks(frame_paths):\n",
    "    \"\"\"\n",
    "    Extract hand landmarks from a sequence of frames\n",
    "    Returns: array of shape (num_frames, 63) - 21 landmarks √ó 3 coordinates per frame\n",
    "    \"\"\"\n",
    "    sequence_landmarks = []\n",
    "    \n",
    "    for frame_path in frame_paths:\n",
    "        image = cv2.imread(frame_path)\n",
    "        if image is None:\n",
    "            # If frame is missing, use zeros\n",
    "            sequence_landmarks.append(np.zeros(63))\n",
    "            continue\n",
    "        \n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(image_rgb)\n",
    "        \n",
    "        if results.multi_hand_landmarks:\n",
    "            # Get first hand\n",
    "            hand = results.multi_hand_landmarks[0]\n",
    "            landmarks = []\n",
    "            for lm in hand.landmark:\n",
    "                landmarks.extend([lm.x, lm.y, lm.z])\n",
    "            sequence_landmarks.append(np.array(landmarks))\n",
    "        else:\n",
    "            # No hand detected, use zeros\n",
    "            sequence_landmarks.append(np.zeros(63))\n",
    "    \n",
    "    return np.array(sequence_landmarks)\n",
    "\n",
    "print(\"‚úÖ Sequence landmark extraction ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä STEP 7: Collect Training Data for Dynamic Signs\n",
    "\n",
    "**We'll record multiple examples of each word**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the words we want to recognize\n",
    "WORDS = ['HELLO', 'THANK_YOU', 'PLEASE', 'YES', 'NO']\n",
    "\n",
    "# Number of examples to collect per word\n",
    "SAMPLES_PER_WORD = 5  # Increase this for better accuracy\n",
    "\n",
    "# Number of frames per sequence\n",
    "SEQUENCE_LENGTH = 30  # 3 seconds at 10 FPS\n",
    "\n",
    "print(\"üìä DATA COLLECTION SETUP\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Words to learn: {WORDS}\")\n",
    "print(f\"Samples per word: {SAMPLES_PER_WORD}\")\n",
    "print(f\"Frames per sample: {SEQUENCE_LENGTH}\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nüí° We'll collect training data by recording you performing each sign\")\n",
    "print(\"   You'll perform each sign multiple times for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect training data\n",
    "X_sequences = []  # Store landmark sequences\n",
    "y_sequences = []  # Store labels\n",
    "\n",
    "print(\"\\nüé¨ STARTING DATA COLLECTION\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nüì∏ When ready, we'll record you performing each sign\")\n",
    "print(\"\\n‚ö†Ô∏è  Make sure:\")\n",
    "print(\"   - Good lighting\")\n",
    "print(\"   - Clear background\")\n",
    "print(\"   - Perform sign smoothly\")\n",
    "print(\"   - Repeat each sign naturally\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "input(\"\\nPress ENTER when ready to start...\")\n",
    "\n",
    "for word in WORDS:\n",
    "    print(f\"\\n\\n{'='*60}\")\n",
    "    print(f\"üìù WORD: {word}\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nüéØ Learn the sign: https://www.handspeak.com/word/search/?id={word.lower()}\")\n",
    "    \n",
    "    for sample in range(SAMPLES_PER_WORD):\n",
    "        print(f\"\\nüìπ Recording sample {sample + 1}/{SAMPLES_PER_WORD} for '{word}'\")\n",
    "        input(f\"   Press ENTER and immediately perform the sign for '{word}'...\")\n",
    "        \n",
    "        # Capture sequence\n",
    "        frames = capture_sequence(num_frames=SEQUENCE_LENGTH, delay_ms=100)\n",
    "        \n",
    "        # Extract landmarks\n",
    "        landmarks = extract_sequence_landmarks(frames)\n",
    "        \n",
    "        # Store\n",
    "        X_sequences.append(landmarks)\n",
    "        y_sequences.append(word)\n",
    "        \n",
    "        print(f\"   ‚úÖ Sample {sample + 1} recorded!\")\n",
    "        \n",
    "        # Cleanup\n",
    "        for frame in frames:\n",
    "            if os.path.exists(frame):\n",
    "                os.remove(frame)\n",
    "\n",
    "# Convert to arrays\n",
    "X_sequences = np.array(X_sequences)\n",
    "y_sequences = np.array(y_sequences)\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ DATA COLLECTION COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìä Total samples: {len(X_sequences)}\")\n",
    "print(f\"üìä Sequence shape: {X_sequences.shape}\")\n",
    "print(f\"   ({X_sequences.shape[0]} samples, {X_sequences.shape[1]} frames, {X_sequences.shape[2]} features)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ STEP 8: Prepare Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y_sequences)\n",
    "y_categorical = keras.utils.to_categorical(y_encoded)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_sequences, y_categorical,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Data prepared!\")\n",
    "print(f\"\\nüìä Training: {len(X_train)} samples\")\n",
    "print(f\"üìä Testing: {len(X_test)} samples\")\n",
    "print(f\"üìä Classes: {label_encoder.classes_}\")\n",
    "\n",
    "# Save label encoder\n",
    "with open('dynamic_label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üß† STEP 9: Build LSTM Model for Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LSTM model for temporal sequences\n",
    "model = keras.Sequential([\n",
    "    # Input: (sequence_length, 63 features)\n",
    "    layers.Input(shape=(SEQUENCE_LENGTH, 63)),\n",
    "    \n",
    "    # LSTM layers to capture temporal patterns\n",
    "    layers.LSTM(128, return_sequences=True),\n",
    "    layers.Dropout(0.3),\n",
    "    \n",
    "    layers.LSTM(256, return_sequences=True),\n",
    "    layers.Dropout(0.3),\n",
    "    \n",
    "    layers.LSTM(128),\n",
    "    layers.Dropout(0.4),\n",
    "    \n",
    "    # Dense layers\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    \n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    \n",
    "    # Output\n",
    "    layers.Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"‚úÖ LSTM Model built!\\n\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üöÄ STEP 10: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=7)\n",
    "]\n",
    "\n",
    "print(\"üöÄ Training LSTM model...\\n\")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=100,\n",
    "    batch_size=8,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìà STEP 11: Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üìä MODEL PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n‚úÖ Accuracy: {test_accuracy*100:.2f}%\")\n",
    "print(f\"‚úÖ Loss: {test_loss:.4f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(history.history['accuracy'], label='Train', linewidth=2)\n",
    "ax1.plot(history.history['val_accuracy'], label='Val', linewidth=2)\n",
    "ax1.set_title('Accuracy', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(history.history['loss'], label='Train', linewidth=2)\n",
    "ax2.plot(history.history['val_loss'], label='Val', linewidth=2)\n",
    "ax2.set_title('Loss', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save\n",
    "model.save('dynamic_asl_model.h5')\n",
    "print(\"\\n‚úÖ Model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ STEP 12: Real-Time Dynamic Sign Recognition!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"üé¨ REAL-TIME DYNAMIC SIGN RECOGNITION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìö Trained words: {list(label_encoder.classes_)}\")\n",
    "print(f\"\\nüí° Perform a sign for {SEQUENCE_LENGTH * 0.1:.1f} seconds\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "input(\"\\nPress ENTER when ready to perform a sign...\")\n",
    "\n",
    "# Capture sequence\n",
    "print(\"\\nüìπ Recording... Perform your sign NOW!\\n\")\n",
    "frames = capture_sequence(num_frames=SEQUENCE_LENGTH, delay_ms=100)\n",
    "\n",
    "# Extract landmarks\n",
    "print(\"\\nüîç Analyzing...\")\n",
    "landmarks = extract_sequence_landmarks(frames)\n",
    "\n",
    "# Predict\n",
    "landmarks = landmarks.reshape(1, SEQUENCE_LENGTH, 63)\n",
    "prediction = model.predict(landmarks, verbose=0)\n",
    "\n",
    "idx = np.argmax(prediction)\n",
    "word = label_encoder.inverse_transform([idx])[0]\n",
    "confidence = prediction[0][idx]\n",
    "\n",
    "# Display result\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéØ PREDICTION RESULT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n‚úÖ Detected Word: {word}\")\n",
    "print(f\"‚úÖ Confidence: {confidence*100:.2f}%\")\n",
    "\n",
    "# Top predictions\n",
    "print(\"\\nüìä All predictions:\")\n",
    "for i in np.argsort(prediction[0])[::-1]:\n",
    "    w = label_encoder.inverse_transform([i])[0]\n",
    "    c = prediction[0][i] * 100\n",
    "    print(f\"   {w}: {c:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üí° Run this cell again to test another sign!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Cleanup\n",
    "for frame in frames:\n",
    "    if os.path.exists(frame):\n",
    "        os.remove(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üíæ STEP 13: Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files.download('dynamic_asl_model.h5')\n",
    "files.download('dynamic_label_encoder.pkl')\n",
    "print(\"‚úÖ Model downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìù Notes\n",
    "\n",
    "### üéØ What We Built:\n",
    "- **Dynamic ASL word recognition** with movement\n",
    "- **LSTM model** to capture temporal patterns\n",
    "- **Sequence-based prediction** (30 frames = ~3 seconds)\n",
    "- Works with words like HELLO, THANK YOU, PLEASE, etc.\n",
    "\n",
    "### üöÄ To Improve:\n",
    "1. **Collect MORE samples** per word (20-50 samples)\n",
    "2. **Add more words** to vocabulary\n",
    "3. **Use pre-recorded dataset** like WLASL for better accuracy\n",
    "4. **Increase sequence length** for longer/complex signs\n",
    "5. **Support 2-hand signs** (already enabled in MediaPipe)\n",
    "\n",
    "### üìö ASL Resources:\n",
    "- Learn signs: https://www.handspeak.com\n",
    "- ASL dictionary: https://www.lifeprint.com\n",
    "- Sign videos: https://www.signingsavvy.com\n",
    "\n",
    "---\n",
    "## üéâ DYNAMIC ASL RECOGNITION COMPLETE!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
